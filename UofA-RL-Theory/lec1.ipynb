{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 導入\n",
    "\n",
    "この講座では，強化学習理論における2つのトピック (1) プランニングと(2) オンライン強化学習を取り扱う．これらのトピックは次のように定義される．\n",
    "\n",
    "* プランニングとは，行動プラン，方策または行動そのものを，何らかの環境モデルを通して計算する問題を指す．\n",
    "* オンライン強化学習とは，長期的な報酬の和を最大化する行動を，環境内で実際に行動しながら見つける問題を指す．\n",
    "\n",
    "本講義では，これら全ての問題に対して，マルコフ決定過程 (Markov Decision Processes 略して MDP) を使って環境のシミュレーションモデルまたは環境自体を表現していく．そこでまず， MDP の定義から始めよう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## マルコフ決定過程\n",
    "マルコフ決定過程 (MDP) とは，確率的な状態遷移が生じる逐次意思決定問題を数学的に定義するモデルである． MDP は，状態集合，行動集合，状態間の確率的遷移のルール，報酬そして目的関数の５つから構築される．\n",
    "本講義では，目的関数として基本的に割引報酬和の期待値 (リターン) を考える．\n",
    "\n",
    "* 状態は抽象的な概念であり，陽に定義しない<a name=\"ref-1\"></a>[<sup>[1]</sup>](#note-1)．\n",
    "状態集合は $\\mathcal{S}$ と表記する．\n",
    "簡単のため，有限集合とする<a name=\"ref-2\"></a>[<sup>[2]</sup>](#note-2)．\n",
    "状態数は $\\mathrm{S}$とする．\n",
    "* 行動もまた抽象的な概念である．行動集合は $\\mathcal{A}$ と表記する．\n",
    "簡単のため，有限集合とする．\n",
    "行動数は $\\mathrm{A}$ と表記する．\n",
    "* 状態 $s$ と $s'$ の間の (確率的) 状態遷移は，行動 $a$ を与えられた状態で行う事で生じる．\n",
    "状態 $s$ と行動 $a$ に対し，あらたな状態 $s'$ に移動する確率をベクトルにまとめ， $P_a(s)$ と表記する<a name=\"ref-3\"></a>[<sup>[3]</sup>](#note-3)．\n",
    "表記を簡単にするため，記法を乱用することになるが，このベクトルの $s'\\in \\mathcal{S}$ に該当する要素を $P_a(s,s')$ と表記する．これは，状態 $s$ で行動 $a$ を行ったとき，次状態 $s'$ に遷移する確率となる．\n",
    "* 報酬は実数値で与えられ，状態 $s$ で行動 $a$ を行ったときの報酬を $r_a(s)$ と表記する．\n",
    "状態と行動の数が有限であるため，報酬が閉区間 $[0,1]$ 内に属するとしても問題はなく，そのように仮定する<a name=\"ref-4\"></a>[<sup>[4]</sup>](#note-4)．\n",
    "\n",
    "時刻 $t$ で行動 $A_t$ を行うとすると，無限に長い軌道 $S_0,A_0,S_1,A_1,...$ が得られる．ここで， $S_{t+1}$ は行動 $A_t$ を時刻 $t\\ge 0$ で行った結果として生じる．また，行動 $A_t$ は過去の履歴 (後ほど定義) にのみ依存し， $S_{t+1}$ は $S_0,A_0,\\dots,S_t,A_t$ が与えられたとき， $P_{A_t}(S_t)$ にのみ ($\\mathbb{P}$-almost surelyで) 依存する．\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}(S_{t+1}=s|S_0,A_0,\\dots,S_t,A_t) = P_{A_t}(S_t,s)\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "我々が考える問題のゴールは，軌道に沿ったリターンが最大化するような行動を選択するルールの発見である．\n",
    "軌道に沿ったリターンは次のように定義される．\n",
    "\n",
    "$$\n",
    "R = r_{A_0}(S_0) + \\gamma r_{A_1}(S_1) + \\gamma^2 r_{A_2}(S_2) + \\dots + \\gamma^t r_{A_t}(S_t) + \\dots\n",
    "$$\n",
    "\n",
    "ここで $\\gamma \\in [0,1)$ は割引率と呼ばれる．\n",
    "正式には， (割引) MDP は $5$つの組 $M = (\\mathcal{S},\\mathcal{A},P,r,\\gamma)$ で定義される．ここで $P=(P_a(s))_{s,a}$ と $r=(r_a(s))_{s,a}$ は確率遷移ベクトルと報酬の組である."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"note-1\"></a>1. [^](#ref-1) 状態集合が有限集合の場合，各要素に1から順に整数を割り当て，整数の集合とみなしてもまったく差し支えない．そのため，状態を陽に定義する必要はない．\n",
    "\n",
    "<a name=\"note-2\"></a>2. [^](#ref-2) 状態集合を有限次元ユークリッド空間のコンパクト部分集合に拡張するのは特に問題は生じない．一方，行動空間を有限次元ユークリッド空間のコンパクト部分集合に拡張する場合， (後ほど出てくる) 最適方策の定義がうまくいかない． MDP 自体に仮定を置くか，可測集合族を拡張する必要がある．前者の詳細は末尾で引用されているPutermanの本に書いてあるため，そちらを参照してほしい．後者については， Bertsekas & Shreve の [Stochastic Optimal Control](http://web.mit.edu/dimitrib/www/soc.html) に書いてあるため，そちらを参照してほしい．\n",
    "\n",
    "<a name=\"note-3\"></a>3. [^](#ref-3) これは次のように考えてほしい．まず，状態 $s$ で行動 $a$ を行ったとき，次状態 $s'$ に遷移する確率を $P_{a} (s, s')$ としよう．これを並べて作ったベクトルが $P_a (s)$ となる．ただし，次状態の並びに関しては，事前に順番を決めておき，その順番に従うものとする．\n",
    "\n",
    "<a name=\"note-4\"></a>4. [^](#ref-4) 有限でない場合は報酬関数に適切な仮定を導入する必要がある．詳しくはPutermanの本を参照してほしい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
